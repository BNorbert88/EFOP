{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diffegy NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TPLa4UbB9XD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ötlet: x_list.grad csak egy skalár vektort ad, requires_grad_=False, nem tudja rendesen figyelembe venni a tanuláshoz\n",
        "#Helyette: közelítő derivált?\n",
        "#Der(1, f, del, x) = (f(x+del/2)-f(x-del/2))/del\n",
        "#Der(2, f, del, x) = (f(x)-f(x-del))/del\n",
        "#Der(3, f, del, x) = (f(x+del)-f(x))/del"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC8HGFsboaoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.autograd\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K2ryasBVV7H",
        "colab_type": "code",
        "outputId": "dd4cb6f6-1b7e-43a4-eb90-1e7c677c8079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "a = th.tensor([1.0], requires_grad=True)\n",
        "b = th.tensor([2.0], requires_grad=True)\n",
        "x = th.tensor([3.0], requires_grad=True)\n",
        "y = a*x*x + b\n",
        "d = torch.autograd.grad([y], [x], only_inputs=True, create_graph=True)[0]\n",
        "w = d + y\n",
        "w.backward()\n",
        "print(a, b, x, y, d, w,\"\\n\")\n",
        "print(a.grad, b.grad, x.grad, y.grad, d.grad, w.grad,\"\\n\")\n",
        "\n",
        "#magasabb foku derivaét?\n",
        "\n",
        "mulg = torch.autograd.grad([y],[a,b,x],only_inputs=True,create_graph=True, retain_graph=True)\n",
        "print(\"mulg:\", mulg)\n",
        "#mulg[1] = 1, hibát ad ha gradot számolunk\n",
        "#print(\"mulg autograd: \", torch.autograd.grad(mulg,[a,b,x],only_inputs=True,create_graph=True, allow_unused=True))\n",
        "print(\"mulg[0] autograd:\",torch.autograd.grad([mulg[0]],[a,b,x],only_inputs=True,create_graph=True, allow_unused=True))\n",
        "print(\"mulg[2] autograd\", torch.autograd.grad([mulg[2]],[a,b,x],only_inputs=True,create_graph=True, allow_unused=True))\n",
        "print(\"[mulg[0],mulg[2]] autograd: \", torch.autograd.grad([mulg[0], mulg[2]],[a,b,x],only_inputs=True,create_graph=True, allow_unused=True))\n",
        "print(\"összeadja\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.], requires_grad=True) tensor([2.], requires_grad=True) tensor([3.], requires_grad=True) tensor([11.], grad_fn=<AddBackward0>) tensor([6.], grad_fn=<AddBackward0>) tensor([17.], grad_fn=<AddBackward0>) \n",
            "\n",
            "tensor([15.]) tensor([1.]) tensor([8.]) None None None \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-48c199826981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#magasabb foku derivaét?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmulg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monly_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mulg:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#mulg[1] = 1, hibát ad ha gradot számolunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yzNxZKWbpaA",
        "colab_type": "code",
        "outputId": "4d9fbff7-81aa-4119-c59c-7e8dd1c8a0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "xdim = th.tensor([1.0, 2.0, 3.01], requires_grad=True)\n",
        "ydim = xdim[0]*xdim[1]*xdim[2] + th.sum(xdim[0:1] ** xdim[0:1])\n",
        "th.autograd.grad(ydim, xdim, only_inputs=True, create_graph=True, retain_graph=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([7.0200, 3.0100, 2.0000], grad_fn=<AddBackward0>),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo9uqSlWfGZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xd = th.tensor([1.0, 2.0], requires_grad=True)\n",
        "yd = th.sum(xd ** xd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgKcp6ci8C93",
        "colab_type": "code",
        "outputId": "f8b130ea-3428-46c6-b83e-c4b415f3d433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#többdim?\n",
        "A = th.tensor([1.0, 2.0],requires_grad=True)\n",
        "B = A + th.tensor([0.0, 1.]) * A * A\n",
        "s = A.size()\n",
        "D = th.autograd.grad(B, A, grad_outputs=th.ones(A.size()), only_inputs=True, create_graph=True)[0]\n",
        "print(D)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 5.], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5Fvqkma242V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 100),\n",
        "    nn.Linear(100, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100,100),\n",
        "    nn.Linear(100,1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mns5SS5-xGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#itt lehet definiálni, milyen differenciál egyenlet megoldását próbáljuk közelíteni, F(t,x,...) = 0 formában a bal oldal kell\n",
        "def ODE(x):\n",
        "  u = model(x)\n",
        "  deriv = th.autograd.grad(u, x, grad_outputs=th.ones(x.size()), only_inputs=True, create_graph=True)[0]  #u'(x)\n",
        "  leftside = deriv - u * th.sin(x) # u'(x)-u(x)sin(x)\n",
        "  return leftside\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac_IRzBKINt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def solution(x):\n",
        "  r = th.cos(x)\n",
        "  r = th.exp(-r)\n",
        "  return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQCFvU___s2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optim(x, lr = 1e-3):\n",
        "  opt = th.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  u = model(x)\n",
        "  deriv = th.autograd.grad(u, x, grad_outputs=th.ones(x.size()), only_inputs=True, create_graph=True)[0]\n",
        "  h = deriv - u * th.sin(x)\n",
        "  h = sum(h*h)\n",
        "  \n",
        "  opt.zero_grad()\n",
        "  h.backward()\n",
        "  opt.step()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o51qf3nsALEA",
        "colab_type": "code",
        "outputId": "843e55c3-892f-43a4-88f8-bef5d64ebd60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "point_count = 10000\n",
        "tan_pont = th.arange(start = 0, end = point_count)\n",
        "tan_pont = tan_pont / (point_count * 1.0)\n",
        "tan_pont.requires_grad = True\n",
        "tan_pont = tan_pont.reshape((point_count,1))\n",
        "testcas = th.sin(2* tan_pont)-2\n",
        "testcas.backward(th.ones(tan_pont.size()))\n",
        "print(tan_pont.requires_grad)\n",
        "print(testcas)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "tensor([[-2.0000],\n",
            "        [-1.9998],\n",
            "        [-1.9996],\n",
            "        ...,\n",
            "        [-1.0905],\n",
            "        [-1.0905],\n",
            "        [-1.0906]], grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeUxKR40M5N0",
        "colab_type": "code",
        "outputId": "0b02c784-8c2f-49a2-e536-92eb3edb186f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_example = th.tensor([2.])\n",
        "x_example.requires_grad = True\n",
        "h = x_example**2-th.sin(x_example)\n",
        "h.backward()\n",
        "x_example.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.4161])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efhvf9iG37jP",
        "colab_type": "code",
        "outputId": "447c06a4-e278-42ff-f64e-0408d86ec99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "#lehet ha először numerikus deriváltal számolunk, gyorsabban közel jut\n",
        "tan_pont.requires_grad_(False)\n",
        "startnum = time.time()\n",
        "for i in range(100):\n",
        "  numoptim(tan_pont, 1e-5 , 1e-6)\n",
        "endnum = time.time()\n",
        "print(endnum - startnum, \"sec\")\n",
        "print(time.gmtime(endnum - startnum))\n",
        "print((endnum - startnum)/100, \"sec on average\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b5507a28ae0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtan_pont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstartnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnumoptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtan_pont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-5\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mendnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach()."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVTiVKICBGkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tan_pont.requires_grad_(True)\n",
        "opt_count = 10\n",
        "start = time.time()\n",
        "for i in range(opt_count):\n",
        "  optim(tan_pont, 1e-7)\n",
        "end = time.time()\n",
        "print(end - start, \"sec\")\n",
        "print(time.gmtime(end - start))\n",
        "print((end - start)/opt_count, \"sec on average\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2VwE66FN0e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotpnt = tan_pont.reshape(tan_pont.size(0))\n",
        "plt.plot(plotpnt.detach().numpy(), solution(plotpnt).detach().numpy())\n",
        "scale = solution(tan_pont[0])/model(tan_pont[0])\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), (scale * model(tan_pont).reshape(tan_pont.size(0))).detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoU4MyxI3ZCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numder(func, x, eps):\n",
        "  x_1 = x - eps\n",
        "  x_2 = x\n",
        "  r = func(x_2)-func(x_1)\n",
        "  r = r / eps\n",
        "  return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1CYeSoz3h_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numoptim(x_list, eps, lr = 1e-3):\n",
        "  opt = th.optim.Adam(model.parameters(), lr=lr) #esetleg más algoritmus?\n",
        "\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  x_grad = numder(model, x_list, eps)\n",
        "  h = x_grad - model(x_list)*th.sin(x_list)\n",
        "  h = sum(h*h)\n",
        "  \n",
        "  h.backward()\n",
        "  opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQzEo_3p2dxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def der(func, x):\n",
        "  x.requires_grad(True)\n",
        "  val = func(x)\n",
        "  d = th.autograd.grad([val], [x],only_inputs=True, create_graph=True)[0]\n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRSimsvfppBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#du/dx - u*sin(x) = 0\n",
        "#ötlet: Adott D(u(x)) = 0, ahol D valami diff-op+függvény pl D(u(x)) = u''(x)-u(x)/(u'(x)-x^2)+sin(e^x)\n",
        "#Ha u: R->R, akkor adott N: R->R neuronháló jó közelítés lehet, ha elég sűrű helyeken teljesíti D(N(x))=0 -->tanítás Sum(x_i, D(N(x_i))^2) hibáfüggvénnyel?\n",
        "\n",
        "#konkrét példa:\n",
        "#ODE: (du/dx)(x) - u(x)*sin(x) = 0 ,tfh x in [0,1], u(0)=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFgZxDcxppQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A neuronháló definiálása\n",
        "model1 = nn.Sequential(\n",
        "    nn.Linear(1, 100),\n",
        "    nn.Linear(100, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100,100),\n",
        "    nn.Linear(100,1)\n",
        ")\n",
        "\n",
        "model2 = nn.Sequential(\n",
        "    nn.Linear(1, 100),\n",
        "    nn.Linear(100, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100,100),\n",
        "    nn.Linear(100,1)\n",
        ")\n",
        "\n",
        "model3 = nn.Sequential(\n",
        "    nn.Linear(1, 100),\n",
        "    nn.Linear(100, 1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1000,1000),\n",
        "    nn.Linear(1000,100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100,100),\n",
        "    nn.Linear(100,1)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klHFL39Sym4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ha lehet, jó lenne az ODE-t úgy definiálnihogy hsználható legyen a hibafüggvényben\n",
        "#def ODE(func, x):\n",
        "#  result = \n",
        "#  return result\n",
        "#lehet csak közelítve tudunk majd számolni\n",
        "#def ODE(der_type, func, x, eps):  \n",
        "#szép deriváltak->talán sympy-vel?\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25vllrzqCEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimalizációs lépés, kézi derivalt kozelites, ebből tudnia kell paraméter-grad-ot számolni\n",
        "# der-type: 1->+/- eps/2, stb\n",
        "def optim(der_type, NN, x_list, DerivEps = 1e-3, lr = 1e-2):\n",
        "  \n",
        "  opt = th.optim.Adam(NN.parameters(), lr=lr) #esetleg más algoritmus?\n",
        "\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  x_grad = deriv(der_type, NN, x_list, DerivEps)\n",
        "  h = x_grad - NN(x_list)*th.sin(x_list)\n",
        "  h = sum(h*h)\n",
        "  \n",
        "  h.backward()\n",
        "  opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypzz1KBCkfM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tanítási pontok, lehetne véletlenszeresíteni is\n",
        "tan_pont = th.tensor(range(1001))*0.001\n",
        "tan_pont = tan_pont.reshape(1001,1)\n",
        "print(tan_pont.size())\n",
        "tan_pont.requires_grad_()\n",
        "tan_pont\n",
        "tan_pont.backward(th.ones(tan_pont.size()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dhUoFwBkLSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tanítás\n",
        "learn_steps = 50\n",
        "start = time.time()\n",
        "for i in range(learn_steps):\n",
        "  optim(1, model1, tan_pont, 1e-4, 1e-7)\n",
        "  optim(2, model2, tan_pont, 1e-4, 1e-7)\n",
        "  optim(3, model3, tan_pont, 1e-4, 1e-7)\n",
        "end = time.time()\n",
        "print(end-start, \"sec\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDm9zDScEVYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), model1(tan_pont).reshape(tan_pont.size(0)).detach().numpy())\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), model2(tan_pont).reshape(tan_pont.size(0)).detach().numpy())\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), model3(tan_pont).reshape(tan_pont.size(0)).detach().numpy())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7VdbdLXIH6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale1 = th.tensor(solution(plotpnt[0]))/model1(tan_pont[0])\n",
        "print(scale1)\n",
        "scale1 = scale1.detach().numpy()\n",
        "plt.plot(plotpnt, solution(plotpnt))\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), scale1*model1(tan_pont).reshape(tan_pont.size(0)).detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbLPvHLOIvem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale2 = th.tensor(solution(plotpnt[0]))/model2(tan_pont[0])\n",
        "print(scale2)\n",
        "scale2 = scale2.detach().numpy()\n",
        "plt.plot(plotpnt, solution(plotpnt))\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), scale2*model2(tan_pont).reshape(tan_pont.size(0)).detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tynrVz2IvwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scale3 = th.tensor(solution(plotpnt[0]))/model3(tan_pont[0])\n",
        "print(scale3)\n",
        "scale3 = scale3.detach().numpy()\n",
        "plt.plot(plotpnt, solution(plotpnt))\n",
        "plt.plot(tan_pont.reshape(tan_pont.size(0)).detach().numpy(), scale3*model3(tan_pont).reshape(tan_pont.size(0)).detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGFvOQyGjjp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#elavult, .grad-os módszer\n",
        "#optimalizációs lépés\n",
        "def optim1(x_list, NN, lr = 1e-2):\n",
        "  \n",
        "  opt = th.optim.Adam(NN.parameters(), lr=lr) #esetleg más algoritmus?\n",
        "  opt.zero_grad()\n",
        "  x_list.grad.data.zero_()\n",
        "  NN(x_list).backward(th.ones(x_list.size()))\n",
        "  x_grad = x_list.grad\n",
        "  h = x_grad - NN(x_list)*th.sin(x_list)\n",
        "  h = sum(h*h)\n",
        "  \n",
        "  opt.zero_grad() # Az x-grad számolás lehet elrontja a súlyok grad-ját\n",
        "  \n",
        "  h.backward()\n",
        "  opt.step()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbko3nPE7YCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#különböző deriváltak számolásához\n",
        "def deriv(der_type, func, x, eps):\n",
        "  if der_type == 1:\n",
        "    x_1 = x - eps/2\n",
        "    x_2 = x + eps/2\n",
        "    r = func(x_2)-func(x_1)\n",
        "    r = r / eps\n",
        "    return r\n",
        "  elif der_type == 2:\n",
        "    x_1 = x - eps\n",
        "    x_2 = x\n",
        "    r = func(x_2)-func(x_1)\n",
        "    r = r / eps\n",
        "    return r\n",
        "  elif der_type == 3:\n",
        "    x_1 = x\n",
        "    x_2 = x + eps\n",
        "    r = func(x_2)-func(x_1)\n",
        "    r = r / eps\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}